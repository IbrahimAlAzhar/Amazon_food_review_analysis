{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D24TweE9Vt-S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import ParameterGrid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y-f57CNWAl7",
        "outputId": "1d1fa951-1b8d-48c7-957f-f6fd1e6ebcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/My Drive/Colab Notebooks/Merged Research Data By PID - Merged Research Data By PID.csv',index_col=0)"
      ],
      "metadata": {
        "id": "_4x--pE1WCWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "v_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
        "\n",
        "for v in v_cols:\n",
        "    sum_col = f'{v}_sum'\n",
        "    df[sum_col] = df[[f'{v}-HAMD{i:02d}' for i in range(1, 18)]].sum(axis=1)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "zjYLP9sDWCSH",
        "outputId": "e61bbbba-46ea-43c7-f7a5-25e753e31458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    UNIQUEID  THERAPY        DRUG  DOSAGE  FREQUENCY  V1-HAMD01  V1-HAMD02  \\\n",
              "0  HMBV-3407  DLX60QD  Duloxetine      60          1          0          0   \n",
              "1  HMBV-3319  DLX60QD  Duloxetine      60          1          1          2   \n",
              "2  HMBV-1609  DLX60QD  Duloxetine      60          1          3          1   \n",
              "3  HMBV-1306  DLX60QD  Duloxetine      60          1          0          1   \n",
              "4  HMBV-3403  DLX60QD  Duloxetine      60          1          0          0   \n",
              "\n",
              "   V1-HAMD03  V1-HAMD04  V1-HAMD05  ...  V9-HAMD17  V1_sum  V2_sum  V3_sum  \\\n",
              "0          0          0          0  ...        NaN       7     9.0     1.0   \n",
              "1          0          1          2  ...        NaN      14     8.0     8.0   \n",
              "2          2          1          2  ...        NaN      22     9.0     9.0   \n",
              "3          0          0          2  ...        NaN       6    12.0     7.0   \n",
              "4          0          1          0  ...        NaN       8    18.0     7.0   \n",
              "\n",
              "   V4_sum  V5_sum  V6_sum  V7_sum  V8_sum  V9_sum  \n",
              "0       2     0.0     0.0     0.0     0.0     0.0  \n",
              "1      19     0.0     0.0     0.0     0.0     0.0  \n",
              "2       7     0.0     0.0     0.0     0.0     0.0  \n",
              "3      11     0.0     0.0     0.0     0.0     0.0  \n",
              "4      10     0.0     0.0     0.0     0.0     0.0  \n",
              "\n",
              "[5 rows x 167 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-4faa12dd-b72e-4f9f-b777-b036659454a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UNIQUEID</th>\n",
              "      <th>THERAPY</th>\n",
              "      <th>DRUG</th>\n",
              "      <th>DOSAGE</th>\n",
              "      <th>FREQUENCY</th>\n",
              "      <th>V1-HAMD01</th>\n",
              "      <th>V1-HAMD02</th>\n",
              "      <th>V1-HAMD03</th>\n",
              "      <th>V1-HAMD04</th>\n",
              "      <th>V1-HAMD05</th>\n",
              "      <th>...</th>\n",
              "      <th>V9-HAMD17</th>\n",
              "      <th>V1_sum</th>\n",
              "      <th>V2_sum</th>\n",
              "      <th>V3_sum</th>\n",
              "      <th>V4_sum</th>\n",
              "      <th>V5_sum</th>\n",
              "      <th>V6_sum</th>\n",
              "      <th>V7_sum</th>\n",
              "      <th>V8_sum</th>\n",
              "      <th>V9_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HMBV-3407</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HMBV-3319</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HMBV-1609</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HMBV-1306</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HMBV-3403</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 167 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4faa12dd-b72e-4f9f-b777-b036659454a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-1a4ed11a-6150-408a-956b-a18bbd5ac63a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a4ed11a-6150-408a-956b-a18bbd5ac63a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-1a4ed11a-6150-408a-956b-a18bbd5ac63a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4faa12dd-b72e-4f9f-b777-b036659454a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4faa12dd-b72e-4f9f-b777-b036659454a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df['LHAMD'] = 0\n",
        "\n",
        "# # Iterate over each row\n",
        "# for index, row in df.iterrows():\n",
        "#     # Find the last non-zero value in the row\n",
        "#     last_non_zero = row[row != 0].iloc[-1]\n",
        "#     # Store the value in a new column 'LastNonZero'\n",
        "#     df.loc[index, 'LHAMD'] = last_non_zero\n",
        "\n",
        "# Find the last non-zero value excluding columns starting with 'V1' for each row\n",
        "last_nonzero_values = df.loc[:, ~df.columns.str.startswith('V1')].apply(lambda row: row[row != 0].iloc[-1], axis=1)\n",
        "\n",
        "# Create a new column 'Last_Nonzero_Value' and store the values\n",
        "df['LHAMD'] = last_nonzero_values.fillna(0)\n",
        "\n",
        "\n",
        "df.loc[df['V1_sum'] < 18, 'FHAMD'] = 'mild'\n",
        "df.loc[(df['V1_sum'] > 17) & (df['V1_sum'] < 25), 'FHAMD'] = 'moderate'\n",
        "df.loc[df['V1_sum'] > 24, 'FHAMD'] = 'severe'\n",
        "\n",
        "df['FHAMD'] = df['V1_sum']\n",
        "df.drop('V1_sum',axis=1,inplace=True)\n",
        "df['DHAMD'] = df['FHAMD']- df['LHAMD']\n",
        "\n",
        "df['FHAMD_level'] = 0\n",
        "df['Difference'] = 0\n",
        "df['Remission'] = 0\n",
        "\n",
        "# FHAMD Groups\n",
        "df.loc[df['FHAMD'] < 18, 'FHAMD_level'] = 'mild'\n",
        "df.loc[(df['FHAMD'] > 17) & (df['FHAMD'] < 25), 'FHAMD_level'] = 'moderate'\n",
        "df.loc[df['FHAMD'] > 24, 'FHAMD_level'] = 'severe'\n",
        "\n",
        "# Difference\n",
        "df.loc[df['DHAMD'] < 0, 'Difference'] = 'Worse'\n",
        "df.loc[df['DHAMD'] == 0, 'Difference'] = 'No change'\n",
        "df.loc[(df['DHAMD'] >= 1) & (df['DHAMD'] <= 15), 'Difference'] = 'Improvement'\n",
        "df.loc[df['DHAMD'] >= 16, 'Difference'] = 'Big Improvement'\n",
        "\n",
        "# Remission\n",
        "df.loc[df['LHAMD'] <= 7, 'Remission'] = 'Full Remission'\n",
        "df.loc[(df['LHAMD'] >= 8) & (df['LHAMD'] <= 17), 'Remission'] = 'Remission'\n",
        "df.loc[(df['LHAMD'] >= 18) & (df['LHAMD'] <= 23), 'Remission'] = 'Partial Remission'\n",
        "df.loc[df['LHAMD'] >= 24, 'Remission'] = 'Mild Remission'\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Custom function to determine label\n",
        "def determine_label(row):\n",
        "    # decrease_threshold = 0.5  # 50% decrease threshold\n",
        "    # increase_threshold = 0.5  # 50% increase threshold\n",
        "\n",
        "    # if row['FHAMD'] >= row['LHAMD']:\n",
        "    #     return 'No Response'\n",
        "    if row['DHAMD'] <= 0:\n",
        "      return 'No Response'\n",
        "    else:\n",
        "      percentage = (row['FHAMD'] - row['LHAMD']) / row['FHAMD']\n",
        "      if percentage < 0.5:\n",
        "        return 'Partial Response'\n",
        "      else:\n",
        "        return 'Full Response'\n",
        "\n",
        "    return 0\n",
        "\n",
        "# Apply the custom function to create the new column 'Response'\n",
        "df['Response'] = df.apply(determine_label, axis=1)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "80C-UuQ7WCO6",
        "outputId": "00963222-513d-405e-f273-c955e09402de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       UNIQUEID  THERAPY        DRUG  DOSAGE  FREQUENCY  V1-HAMD01  V1-HAMD02  \\\n",
              "0     HMBV-3407  DLX60QD  Duloxetine      60          1          0          0   \n",
              "1     HMBV-3319  DLX60QD  Duloxetine      60          1          1          2   \n",
              "2     HMBV-1609  DLX60QD  Duloxetine      60          1          3          1   \n",
              "3     HMBV-1306  DLX60QD  Duloxetine      60          1          0          1   \n",
              "4     HMBV-3403  DLX60QD  Duloxetine      60          1          0          0   \n",
              "...         ...      ...         ...     ...        ...        ...        ...   \n",
              "1463   HMAQb-30  FLX20QD  Fluoxetine      20          1          3          2   \n",
              "1464  HMAQb-139  FLX20QD  Fluoxetine      20          1          3          2   \n",
              "1465   HMAQb-39  FLX20QD  Fluoxetine      20          1          2          1   \n",
              "1466    HMAQb-6  FLX20QD  Fluoxetine      20          1          2          1   \n",
              "1467   HMAQb-45  FLX20QD  Fluoxetine      20          1          3          3   \n",
              "\n",
              "      V1-HAMD03  V1-HAMD04  V1-HAMD05  ...  V7_sum  V8_sum  V9_sum  LHAMD  \\\n",
              "0             0          0          0  ...     0.0     0.0     0.0    2.0   \n",
              "1             0          1          2  ...     0.0     0.0     0.0   19.0   \n",
              "2             2          1          2  ...     0.0     0.0     0.0    7.0   \n",
              "3             0          0          2  ...     0.0     0.0     0.0   11.0   \n",
              "4             0          1          0  ...     0.0     0.0     0.0   10.0   \n",
              "...         ...        ...        ...  ...     ...     ...     ...    ...   \n",
              "1463          2          2          2  ...    14.0     7.0     9.0    9.0   \n",
              "1464          0          2          2  ...     4.0     8.0     7.0    7.0   \n",
              "1465          0          1          1  ...    19.0    15.0    15.0   15.0   \n",
              "1466          0          0          1  ...    19.0    13.0    11.0   11.0   \n",
              "1467          0          2          2  ...     7.0     3.0     4.0    4.0   \n",
              "\n",
              "      FHAMD  DHAMD  FHAMD_level       Difference          Remission  \\\n",
              "0         7    5.0         mild      Improvement     Full Remission   \n",
              "1        14   -5.0         mild            Worse  Partial Remission   \n",
              "2        22   15.0     moderate      Improvement     Full Remission   \n",
              "3         6   -5.0         mild            Worse          Remission   \n",
              "4         8   -2.0         mild            Worse          Remission   \n",
              "...     ...    ...          ...              ...                ...   \n",
              "1463     27   18.0       severe  Big Improvement          Remission   \n",
              "1464     26   19.0       severe  Big Improvement     Full Remission   \n",
              "1465     13   -2.0         mild            Worse          Remission   \n",
              "1466     20    9.0     moderate      Improvement          Remission   \n",
              "1467     28   24.0       severe  Big Improvement     Full Remission   \n",
              "\n",
              "              Response  \n",
              "0        Full Response  \n",
              "1          No Response  \n",
              "2        Full Response  \n",
              "3          No Response  \n",
              "4          No Response  \n",
              "...                ...  \n",
              "1463     Full Response  \n",
              "1464     Full Response  \n",
              "1465       No Response  \n",
              "1466  Partial Response  \n",
              "1467     Full Response  \n",
              "\n",
              "[1468 rows x 173 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-c7b1403b-e9c4-408f-b7e8-1ab708f38721\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UNIQUEID</th>\n",
              "      <th>THERAPY</th>\n",
              "      <th>DRUG</th>\n",
              "      <th>DOSAGE</th>\n",
              "      <th>FREQUENCY</th>\n",
              "      <th>V1-HAMD01</th>\n",
              "      <th>V1-HAMD02</th>\n",
              "      <th>V1-HAMD03</th>\n",
              "      <th>V1-HAMD04</th>\n",
              "      <th>V1-HAMD05</th>\n",
              "      <th>...</th>\n",
              "      <th>V7_sum</th>\n",
              "      <th>V8_sum</th>\n",
              "      <th>V9_sum</th>\n",
              "      <th>LHAMD</th>\n",
              "      <th>FHAMD</th>\n",
              "      <th>DHAMD</th>\n",
              "      <th>FHAMD_level</th>\n",
              "      <th>Difference</th>\n",
              "      <th>Remission</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HMBV-3407</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>mild</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Full Remission</td>\n",
              "      <td>Full Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HMBV-3319</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>14</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>mild</td>\n",
              "      <td>Worse</td>\n",
              "      <td>Partial Remission</td>\n",
              "      <td>No Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HMBV-1609</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>22</td>\n",
              "      <td>15.0</td>\n",
              "      <td>moderate</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Full Remission</td>\n",
              "      <td>Full Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HMBV-1306</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>mild</td>\n",
              "      <td>Worse</td>\n",
              "      <td>Remission</td>\n",
              "      <td>No Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HMBV-3403</td>\n",
              "      <td>DLX60QD</td>\n",
              "      <td>Duloxetine</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>mild</td>\n",
              "      <td>Worse</td>\n",
              "      <td>Remission</td>\n",
              "      <td>No Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>HMAQb-30</td>\n",
              "      <td>FLX20QD</td>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>27</td>\n",
              "      <td>18.0</td>\n",
              "      <td>severe</td>\n",
              "      <td>Big Improvement</td>\n",
              "      <td>Remission</td>\n",
              "      <td>Full Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>HMAQb-139</td>\n",
              "      <td>FLX20QD</td>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>26</td>\n",
              "      <td>19.0</td>\n",
              "      <td>severe</td>\n",
              "      <td>Big Improvement</td>\n",
              "      <td>Full Remission</td>\n",
              "      <td>Full Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>HMAQb-39</td>\n",
              "      <td>FLX20QD</td>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>19.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>mild</td>\n",
              "      <td>Worse</td>\n",
              "      <td>Remission</td>\n",
              "      <td>No Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>HMAQb-6</td>\n",
              "      <td>FLX20QD</td>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>19.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20</td>\n",
              "      <td>9.0</td>\n",
              "      <td>moderate</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Remission</td>\n",
              "      <td>Partial Response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>HMAQb-45</td>\n",
              "      <td>FLX20QD</td>\n",
              "      <td>Fluoxetine</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>28</td>\n",
              "      <td>24.0</td>\n",
              "      <td>severe</td>\n",
              "      <td>Big Improvement</td>\n",
              "      <td>Full Remission</td>\n",
              "      <td>Full Response</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1468 rows × 173 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7b1403b-e9c4-408f-b7e8-1ab708f38721')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ea360c47-f5ba-49b3-b4ed-81c40940c7ad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea360c47-f5ba-49b3-b4ed-81c40940c7ad')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ea360c47-f5ba-49b3-b4ed-81c40940c7ad button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7b1403b-e9c4-408f-b7e8-1ab708f38721 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7b1403b-e9c4-408f-b7e8-1ab708f38721');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_worse = (df['Difference'] == 'Worse').sum()\n",
        "print('count_worse',count_worse)\n",
        "count_no_change = (df['Difference'] == 'No change').sum()\n",
        "print('count_no_change',count_no_change)\n",
        "count_improvement = (df['Difference'] == 'Improvement').sum()\n",
        "print('count_improvement',count_improvement)\n",
        "count_big_improvement = (df['Difference'] == 'Big Improvement').sum()\n",
        "print('count_big_improvement',count_big_improvement)\n",
        "count_full_remission = (df['Remission'] == 'Full Remission').sum()\n",
        "print('count_full_remission',count_full_remission)\n",
        "count_remission = (df['Remission'] == 'Remission').sum()\n",
        "print('count_remission',count_remission)\n",
        "count_partial_remission = (df['Remission'] == 'Partial Remission').sum()\n",
        "print('count_partial_remission',count_partial_remission)\n",
        "count_mild_remission = (df['Remission'] == 'Mild Remission').sum()\n",
        "print('count_mild_remission',count_mild_remission)\n",
        "\n",
        "count_response = (df['Response'] == 'No Response').sum()\n",
        "print('count_response',count_response)\n",
        "count_partial_response = (df['Response'] == 'Partial Response').sum()\n",
        "print('count_partial_response',count_partial_response)\n",
        "count_full_response = (df['Response'] == 'Full Response').sum()\n",
        "print('count_full_response',count_full_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLIqhjDZEAiG",
        "outputId": "06ed0ada-6d79-442c-8328-f690cd5d0749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count_worse 95\n",
            "count_no_change 36\n",
            "count_improvement 1108\n",
            "count_big_improvement 229\n",
            "count_full_remission 686\n",
            "count_remission 641\n",
            "count_partial_remission 113\n",
            "count_mild_remission 28\n",
            "count_response 131\n",
            "count_partial_response 499\n",
            "count_full_response 838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n"
      ],
      "metadata": {
        "id": "lsxKo-BTWbbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "zsc73QzoZcpI",
        "outputId": "196dba3b-d8d3-4c15-9e56-fc2016c7a024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      UNIQUEID  THERAPY  DRUG  DOSAGE  FREQUENCY  V1-HAMD01  V1-HAMD02  \\\n",
              "0          768        2     0      60          1          0          0   \n",
              "1          764        2     0      60          1          1          2   \n",
              "2          712        2     0      60          1          3          1   \n",
              "3          701        2     0      60          1          0          1   \n",
              "4          766        2     0      60          1          0          0   \n",
              "...        ...      ...   ...     ...        ...        ...        ...   \n",
              "1463        31        3     1      20          1          3          2   \n",
              "1464        23        3     1      20          1          3          2   \n",
              "1465        32        3     1      20          1          2          1   \n",
              "1466        34        3     1      20          1          2          1   \n",
              "1467        33        3     1      20          1          3          3   \n",
              "\n",
              "      V1-HAMD03  V1-HAMD04  V1-HAMD05  ...  V7_sum  V8_sum  V9_sum  LHAMD  \\\n",
              "0             0          0          0  ...     0.0     0.0     0.0    2.0   \n",
              "1             0          1          2  ...     0.0     0.0     0.0   19.0   \n",
              "2             2          1          2  ...     0.0     0.0     0.0    7.0   \n",
              "3             0          0          2  ...     0.0     0.0     0.0   11.0   \n",
              "4             0          1          0  ...     0.0     0.0     0.0   10.0   \n",
              "...         ...        ...        ...  ...     ...     ...     ...    ...   \n",
              "1463          2          2          2  ...    14.0     7.0     9.0    9.0   \n",
              "1464          0          2          2  ...     4.0     8.0     7.0    7.0   \n",
              "1465          0          1          1  ...    19.0    15.0    15.0   15.0   \n",
              "1466          0          0          1  ...    19.0    13.0    11.0   11.0   \n",
              "1467          0          2          2  ...     7.0     3.0     4.0    4.0   \n",
              "\n",
              "      FHAMD  DHAMD  FHAMD_level  Difference  Remission  Response  \n",
              "0         7    5.0            0           1          0         0  \n",
              "1        14   -5.0            0           3          2         1  \n",
              "2        22   15.0            1           1          0         0  \n",
              "3         6   -5.0            0           3          3         1  \n",
              "4         8   -2.0            0           3          3         1  \n",
              "...     ...    ...          ...         ...        ...       ...  \n",
              "1463     27   18.0            2           0          3         0  \n",
              "1464     26   19.0            2           0          0         0  \n",
              "1465     13   -2.0            0           3          3         1  \n",
              "1466     20    9.0            1           1          3         2  \n",
              "1467     28   24.0            2           0          0         0  \n",
              "\n",
              "[1468 rows x 173 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-47ad89d7-dcd5-4a6d-9309-a71643926a66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UNIQUEID</th>\n",
              "      <th>THERAPY</th>\n",
              "      <th>DRUG</th>\n",
              "      <th>DOSAGE</th>\n",
              "      <th>FREQUENCY</th>\n",
              "      <th>V1-HAMD01</th>\n",
              "      <th>V1-HAMD02</th>\n",
              "      <th>V1-HAMD03</th>\n",
              "      <th>V1-HAMD04</th>\n",
              "      <th>V1-HAMD05</th>\n",
              "      <th>...</th>\n",
              "      <th>V7_sum</th>\n",
              "      <th>V8_sum</th>\n",
              "      <th>V9_sum</th>\n",
              "      <th>LHAMD</th>\n",
              "      <th>FHAMD</th>\n",
              "      <th>DHAMD</th>\n",
              "      <th>FHAMD_level</th>\n",
              "      <th>Difference</th>\n",
              "      <th>Remission</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>768</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>764</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>14</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>712</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>22</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>701</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>766</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>31</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>27</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>26</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>19.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>34</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>19.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>33</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>28</td>\n",
              "      <td>24.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1468 rows × 173 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47ad89d7-dcd5-4a6d-9309-a71643926a66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-9564281b-31b3-4e52-9339-d547a5290530\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9564281b-31b3-4e52-9339-d547a5290530')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-9564281b-31b3-4e52-9339-d547a5290530 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47ad89d7-dcd5-4a6d-9309-a71643926a66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47ad89d7-dcd5-4a6d-9309-a71643926a66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "dWpa5OehWoP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Data (F1 Score)**"
      ],
      "metadata": {
        "id": "Fe2gd6k5WqnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "classifier_1 = SVC()\n",
        "classifier_2 = SVC()\n",
        "classifier_3 = SVC()\n",
        "classifier_4 = SVC()\n",
        "\n",
        "# Train the classifiers independently\n",
        "classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPyQofIWWCMH",
        "outputId": "918dce4b-5622-481b-9bbd-07915a7eb6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7887563884156729\n",
            "Macro Precision: 0.6954225352112676\n",
            "Weighted Precision: 0.8067639225472083\n",
            "Micro Recall: 0.7887563884156729\n",
            "Macro Recall: 0.34194756554307115\n",
            "Weighted Recall: 0.7887563884156729\n",
            "Micro F1 Score: 0.7887563884156729\n",
            "Macro F1 Score: 0.3705491141744819\n",
            "Weighted F1 Score: 0.7184552817791903\n",
            "Average F1 Score: 0.6259202614564483\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.8475298126064736\n",
            "Macro Precision: 0.8641769747014508\n",
            "Weighted Precision: 0.8492087404505477\n",
            "Micro Recall: 0.8475298126064736\n",
            "Macro Recall: 0.6558247611058787\n",
            "Weighted Recall: 0.8475298126064736\n",
            "Micro F1 Score: 0.8475298126064736\n",
            "Macro F1 Score: 0.712401502147893\n",
            "Weighted F1 Score: 0.8427451895510129\n",
            "Average F1 Score: 0.8008921681017932\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.8185689948892675\n",
            "Macro Precision: 0.856719256028799\n",
            "Weighted Precision: 0.8257616905789502\n",
            "Micro Recall: 0.8185689948892675\n",
            "Macro Recall: 0.6641117541577927\n",
            "Weighted Recall: 0.8185689948892675\n",
            "Micro F1 Score: 0.8185689948892675\n",
            "Macro F1 Score: 0.7015673344562711\n",
            "Weighted F1 Score: 0.8043965401923834\n",
            "Average F1 Score: 0.7748442898459741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "viILWN2tYig7",
        "outputId": "2e69bdaa-541c-46e0-bf1a-9d4dfeec2ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      FHAMD_level  Difference  Remission  Response\n",
              "654             1           1          3         0\n",
              "1338            1           0          0         0\n",
              "254             1           1          3         2\n",
              "1070            1           0          0         0\n",
              "1280            1           1          0         0\n",
              "...           ...         ...        ...       ...\n",
              "1130            1           1          3         2\n",
              "1294            1           1          3         0\n",
              "860             0           1          0         0\n",
              "1459            0           3          2         1\n",
              "1126            1           1          3         2\n",
              "\n",
              "[1174 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-db91e0dc-c64c-4262-8924-5289c16feebe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FHAMD_level</th>\n",
              "      <th>Difference</th>\n",
              "      <th>Remission</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1338</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1174 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db91e0dc-c64c-4262-8924-5289c16feebe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-3c714b66-1b88-4635-ad0d-daa7f2c301c6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c714b66-1b88-4635-ad0d-daa7f2c301c6')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-3c714b66-1b88-4635-ad0d-daa7f2c301c6 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db91e0dc-c64c-4262-8924-5289c16feebe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db91e0dc-c64c-4262-8924-5289c16feebe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Accuracy**"
      ],
      "metadata": {
        "id": "P-JxTZqp8L_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB5A8ata8Lhd",
        "outputId": "33d5ea8e-7c13-45df-848b-0407682aeafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9514480408858603\n",
            "Accuracy: 0.9599659284497445\n",
            "Accuracy: 0.975298126064736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train['Difference'][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObRvb9XR1Gcd",
        "outputId": "0204a343-4f89-4601-a0d5-85560673d2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "654     1\n",
              "1338    0\n",
              "254     1\n",
              "1070    0\n",
              "1280    1\n",
              "889     1\n",
              "712     2\n",
              "707     0\n",
              "86      1\n",
              "1392    1\n",
              "265     1\n",
              "621     1\n",
              "1452    1\n",
              "1445    1\n",
              "1105    1\n",
              "514     1\n",
              "705     1\n",
              "221     1\n",
              "113     1\n",
              "1372    0\n",
              "Name: Difference, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred['Label_3'][:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNUI8L_11MKO",
        "outputId": "36f82d48-e673-4ac2-c8b7-7cbe171a3e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     3\n",
              "1     0\n",
              "2     3\n",
              "3     0\n",
              "4     0\n",
              "5     0\n",
              "6     3\n",
              "7     0\n",
              "8     3\n",
              "9     0\n",
              "10    3\n",
              "11    0\n",
              "12    3\n",
              "13    3\n",
              "14    3\n",
              "15    0\n",
              "16    0\n",
              "17    2\n",
              "18    3\n",
              "19    0\n",
              "20    0\n",
              "21    3\n",
              "22    0\n",
              "23    3\n",
              "24    3\n",
              "25    0\n",
              "26    0\n",
              "27    3\n",
              "28    0\n",
              "29    0\n",
              "Name: Label_3, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 Score (Test Data)**"
      ],
      "metadata": {
        "id": "oS-xwC9Tddyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "classifier_1 = SVC()\n",
        "classifier_2 = SVC()\n",
        "classifier_3 = SVC()\n",
        "classifier_4 = SVC()\n",
        "\n",
        "# Train the classifiers independently\n",
        "classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49lLw783dda2",
        "outputId": "b5ab218c-b014-4fd8-ce8a-ca9c55ca4269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7721088435374149\n",
            "Macro Precision: 0.6916376306620209\n",
            "Weighted Precision: 0.8151058332740763\n",
            "Micro Recall: 0.7721088435374149\n",
            "Macro Recall: 0.30710784313725487\n",
            "Weighted Recall: 0.7721088435374149\n",
            "Micro F1 Score: 0.7721088435374149\n",
            "Macro F1 Score: 0.31854355232281656\n",
            "Weighted F1 Score: 0.6923897782951772\n",
            "Average F1 Score: 0.5943473913851363\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.7959183673469388\n",
            "Macro Precision: 0.8143651786955917\n",
            "Weighted Precision: 0.8002823824565286\n",
            "Micro Recall: 0.7959183673469388\n",
            "Macro Recall: 0.5890383504261159\n",
            "Weighted Recall: 0.7959183673469388\n",
            "Micro F1 Score: 0.7959183673469388\n",
            "Macro F1 Score: 0.6199398574398576\n",
            "Weighted F1 Score: 0.7894375481110174\n",
            "Average F1 Score: 0.7350985909659379\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7891156462585034\n",
            "Macro Precision: 0.8437282714504241\n",
            "Weighted Precision: 0.8002449206633007\n",
            "Micro Recall: 0.7891156462585034\n",
            "Macro Recall: 0.6001375840908899\n",
            "Weighted Recall: 0.7891156462585034\n",
            "Micro F1 Score: 0.7891156462585034\n",
            "Macro F1 Score: 0.6242995525086324\n",
            "Weighted F1 Score: 0.770650361808266\n",
            "Average F1 Score: 0.7280218535251338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy**"
      ],
      "metadata": {
        "id": "_aaTaoaU8jyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sG4MJxq8iwW",
        "outputId": "7d33501b-426a-44f6-96cc-01e7f7c6ba52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7721088435374149\n",
            "Accuracy: 0.7959183673469388\n",
            "Accuracy: 0.7891156462585034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "qPPYFJmgjTdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = LogisticRegression()\n",
        "classifier_3 = LogisticRegression()\n",
        "classifier_4 = LogisticRegression()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmyUbzNaerC-",
        "outputId": "0c5f1d50-daec-4f68-86d0-5c503750cb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.9063032367972743\n",
            "Macro Precision: 0.8740401785714286\n",
            "Weighted Precision: 0.9033273758822097\n",
            "Micro Recall: 0.9063032367972743\n",
            "Macro Recall: 0.7119987760998997\n",
            "Weighted Recall: 0.9063032367972743\n",
            "Micro F1 Score: 0.9063032367972743\n",
            "Macro F1 Score: 0.7695557716933714\n",
            "Weighted F1 Score: 0.900608031880966\n",
            "Average F1 Score: 0.8588223467905373\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.8321976149914821\n",
            "Macro Precision: 0.8175578650584883\n",
            "Weighted Precision: 0.8302302069708805\n",
            "Micro Recall: 0.8321976149914821\n",
            "Macro Recall: 0.7539592679916916\n",
            "Weighted Recall: 0.8321976149914821\n",
            "Micro F1 Score: 0.8321976149914821\n",
            "Macro F1 Score: 0.7805082748758069\n",
            "Weighted F1 Score: 0.8295512991574465\n",
            "Average F1 Score: 0.8140857296749119\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.8517887563884157\n",
            "Macro Precision: 0.8665221368182209\n",
            "Weighted Precision: 0.8512500042937459\n",
            "Micro Recall: 0.8517887563884157\n",
            "Macro Recall: 0.8373667702748234\n",
            "Weighted Recall: 0.8517887563884157\n",
            "Micro F1 Score: 0.8517887563884157\n",
            "Macro F1 Score: 0.8509327501632876\n",
            "Weighted F1 Score: 0.8509076303417825\n",
            "Average F1 Score: 0.8512097122978286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Accuracy**"
      ],
      "metadata": {
        "id": "CMvHWChE8sAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYYIHJTJ8rpD",
        "outputId": "043a430e-1b7d-4f8c-e262-7aaec915dd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9063032367972743\n",
            "Accuracy: 0.8321976149914821\n",
            "Accuracy: 0.8517887563884157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = LogisticRegression()\n",
        "classifier_3 = LogisticRegression()\n",
        "classifier_4 = LogisticRegression()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxkVlxsyeq_n",
        "outputId": "2df4e1df-ab07-4319-ac09-68d34e46d85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.8027210884353742\n",
            "Macro Precision: 0.5278846153846154\n",
            "Weighted Precision: 0.7907014942729229\n",
            "Micro Recall: 0.8027210884353742\n",
            "Macro Recall: 0.4910873440285205\n",
            "Weighted Recall: 0.8027210884353742\n",
            "Micro F1 Score: 0.802721088435374\n",
            "Macro F1 Score: 0.5067569805571197\n",
            "Weighted F1 Score: 0.7945076212062117\n",
            "Average F1 Score: 0.7013285633995685\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.7040816326530612\n",
            "Macro Precision: 0.49444444444444446\n",
            "Weighted Precision: 0.7014361300075586\n",
            "Micro Recall: 0.7040816326530612\n",
            "Macro Recall: 0.4971869131879243\n",
            "Weighted Recall: 0.7040816326530612\n",
            "Micro F1 Score: 0.7040816326530612\n",
            "Macro F1 Score: 0.49351898101898106\n",
            "Weighted F1 Score: 0.7021371485657201\n",
            "Average F1 Score: 0.6332459207459208\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7619047619047619\n",
            "Macro Precision: 0.6970423797049183\n",
            "Weighted Precision: 0.7544574799266149\n",
            "Micro Recall: 0.7619047619047619\n",
            "Macro Recall: 0.6516058511221966\n",
            "Weighted Recall: 0.7619047619047619\n",
            "Micro F1 Score: 0.7619047619047619\n",
            "Macro F1 Score: 0.6699520717377861\n",
            "Weighted F1 Score: 0.7564974505572173\n",
            "Average F1 Score: 0.7294514280665885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy**"
      ],
      "metadata": {
        "id": "s0fxV7DE847j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5ht44JafMp8",
        "outputId": "7510b2d9-9cc0-4433-fe90-2b9307162a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8027210884353742\n",
            "Accuracy: 0.7040816326530612\n",
            "Accuracy: 0.7619047619047619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "6dRB3qoOjzAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = GaussianNB()\n",
        "classifier_3 = GaussianNB()\n",
        "classifier_4 = GaussianNB()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLEQvzJ7eq5c",
        "outputId": "27c7c11f-793e-42bb-ac89-6239bf5de410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.17206132879045996\n",
            "Macro Precision: 0.40490430622009566\n",
            "Weighted Precision: 0.7446895657915115\n",
            "Micro Recall: 0.17206132879045996\n",
            "Macro Recall: 0.3361560909735067\n",
            "Weighted Recall: 0.17206132879045996\n",
            "Micro F1 Score: 0.17206132879045996\n",
            "Macro F1 Score: 0.17131953336178685\n",
            "Weighted F1 Score: 0.07667167148607751\n",
            "Average F1 Score: 0.14001751121277478\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.5187393526405452\n",
            "Macro Precision: 0.4896434346434346\n",
            "Weighted Precision: 0.6352580948322004\n",
            "Micro Recall: 0.5187393526405452\n",
            "Macro Recall: 0.536988463977023\n",
            "Weighted Recall: 0.5187393526405452\n",
            "Micro F1 Score: 0.5187393526405452\n",
            "Macro F1 Score: 0.39701132878899326\n",
            "Weighted F1 Score: 0.4057704895182632\n",
            "Average F1 Score: 0.44050705698260056\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.6490630323679727\n",
            "Macro Precision: 0.5479598956905479\n",
            "Weighted Precision: 0.6390744758751156\n",
            "Micro Recall: 0.6490630323679727\n",
            "Macro Recall: 0.4413433508935971\n",
            "Weighted Recall: 0.6490630323679727\n",
            "Micro F1 Score: 0.6490630323679727\n",
            "Macro F1 Score: 0.43781812808482146\n",
            "Weighted F1 Score: 0.5953875307498383\n",
            "Average F1 Score: 0.5607562304008775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Accuracy**"
      ],
      "metadata": {
        "id": "19Lj1XKk9IyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE8b-Orh9H09",
        "outputId": "3dcac54f-f729-4564-fdb9-9103366f8683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.17206132879045996\n",
            "Accuracy: 0.5187393526405452\n",
            "Accuracy: 0.6490630323679727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = GaussianNB()\n",
        "classifier_3 = GaussianNB()\n",
        "classifier_4 = GaussianNB()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpvmwpLeqzU",
        "outputId": "abf97861-1f44-4073-9376-33a791485790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.18027210884353742\n",
            "Macro Precision: 0.2107843137254902\n",
            "Weighted Precision: 0.3238378684807256\n",
            "Micro Recall: 0.18027210884353742\n",
            "Macro Recall: 0.33453654188948306\n",
            "Weighted Recall: 0.18027210884353742\n",
            "Micro F1 Score: 0.18027210884353745\n",
            "Macro F1 Score: 0.14556766762649115\n",
            "Weighted F1 Score: 0.07833770239785277\n",
            "Average F1 Score: 0.13472582628929378\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.4965986394557823\n",
            "Macro Precision: 0.3516154324977855\n",
            "Weighted Precision: 0.42114485191716083\n",
            "Micro Recall: 0.4965986394557823\n",
            "Macro Recall: 0.5179287640233039\n",
            "Weighted Recall: 0.4965986394557823\n",
            "Micro F1 Score: 0.4965986394557823\n",
            "Macro F1 Score: 0.3666061615570917\n",
            "Weighted F1 Score: 0.38048612067768195\n",
            "Average F1 Score: 0.41456364056351863\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.6462585034013606\n",
            "Macro Precision: 0.5393732193732194\n",
            "Weighted Precision: 0.6307494621780335\n",
            "Micro Recall: 0.6462585034013606\n",
            "Macro Recall: 0.45434993627239967\n",
            "Weighted Recall: 0.6462585034013606\n",
            "Micro F1 Score: 0.6462585034013606\n",
            "Macro F1 Score: 0.4604268373857254\n",
            "Weighted F1 Score: 0.6052090950411435\n",
            "Average F1 Score: 0.5706314786094099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy**"
      ],
      "metadata": {
        "id": "mGyxS_y-9-oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BST6xmlkfNP-",
        "outputId": "55c66fe7-7799-4f53-d82e-000bc1523b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.18027210884353742\n",
            "Accuracy: 0.4965986394557823\n",
            "Accuracy: 0.6462585034013606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**"
      ],
      "metadata": {
        "id": "6QSsFKzHkDZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = KNeighborsClassifier()\n",
        "classifier_3 = KNeighborsClassifier()\n",
        "classifier_4 = KNeighborsClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL8ZBHZOeqsx",
        "outputId": "33bd2e05-f2fe-4b45-914a-584154b60863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.8347529812606473\n",
            "Macro Precision: 0.6469489120151372\n",
            "Weighted Precision: 0.8195105558949102\n",
            "Micro Recall: 0.8347529812606473\n",
            "Macro Recall: 0.44830828019030267\n",
            "Weighted Recall: 0.8347529812606473\n",
            "Micro F1 Score: 0.8347529812606473\n",
            "Macro F1 Score: 0.5016054609586026\n",
            "Weighted F1 Score: 0.8023585878219498\n",
            "Average F1 Score: 0.7129056766803998\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.9224872231686542\n",
            "Macro Precision: 0.9168562648500311\n",
            "Weighted Precision: 0.9231730446680781\n",
            "Micro Recall: 0.9224872231686542\n",
            "Macro Recall: 0.7776047119570397\n",
            "Weighted Recall: 0.9224872231686542\n",
            "Micro F1 Score: 0.9224872231686542\n",
            "Macro F1 Score: 0.8186298252499634\n",
            "Weighted F1 Score: 0.9207429678529233\n",
            "Average F1 Score: 0.8872866720905136\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.873083475298126\n",
            "Macro Precision: 0.8572257427581466\n",
            "Weighted Precision: 0.8735623354489045\n",
            "Micro Recall: 0.873083475298126\n",
            "Macro Recall: 0.7593295634675187\n",
            "Weighted Recall: 0.873083475298126\n",
            "Micro F1 Score: 0.873083475298126\n",
            "Macro F1 Score: 0.7889202848223835\n",
            "Weighted F1 Score: 0.8675497852881001\n",
            "Average F1 Score: 0.8431845151362033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E6ozL5y-KLx",
        "outputId": "658650cd-0fe3-429f-f541-b6289e94758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8347529812606473\n",
            "Accuracy: 0.9224872231686542\n",
            "Accuracy: 0.873083475298126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = KNeighborsClassifier()\n",
        "classifier_3 = KNeighborsClassifier()\n",
        "classifier_4 = KNeighborsClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG4e-xnNeqmY",
        "outputId": "615feb2e-6776-4832-e048-d2270ad4b224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7789115646258503\n",
            "Macro Precision: 0.5191658391261172\n",
            "Weighted Precision: 0.752497821372839\n",
            "Micro Recall: 0.7789115646258503\n",
            "Macro Recall: 0.3773618538324421\n",
            "Weighted Recall: 0.7789115646258503\n",
            "Micro F1 Score: 0.7789115646258504\n",
            "Macro F1 Score: 0.4093519882179676\n",
            "Weighted F1 Score: 0.7396541532666085\n",
            "Average F1 Score: 0.6426392353701421\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.8639455782312925\n",
            "Macro Precision: 0.8567516208361279\n",
            "Weighted Precision: 0.8687179767950105\n",
            "Micro Recall: 0.8639455782312925\n",
            "Macro Recall: 0.7128677355674322\n",
            "Weighted Recall: 0.8639455782312925\n",
            "Micro F1 Score: 0.8639455782312925\n",
            "Macro F1 Score: 0.7336155674390968\n",
            "Weighted F1 Score: 0.8610567459306955\n",
            "Average F1 Score: 0.8195392972003616\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.8061224489795918\n",
            "Macro Precision: 0.7318131868131869\n",
            "Weighted Precision: 0.8002214622112581\n",
            "Micro Recall: 0.8061224489795918\n",
            "Macro Recall: 0.6779595350839432\n",
            "Weighted Recall: 0.8061224489795918\n",
            "Micro F1 Score: 0.806122448979592\n",
            "Macro F1 Score: 0.697449349779539\n",
            "Weighted F1 Score: 0.801291784224992\n",
            "Average F1 Score: 0.7682878609947076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esFoglMufOQm",
        "outputId": "a05adeb8-b681-44fc-fb82-8af8b6fe2828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7789115646258503\n",
            "Accuracy: 0.8639455782312925\n",
            "Accuracy: 0.8061224489795918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forests**"
      ],
      "metadata": {
        "id": "H15mgRevkbk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = RandomForestClassifier()\n",
        "classifier_3 = RandomForestClassifier()\n",
        "classifier_4 = RandomForestClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-hxEOh0eqgG",
        "outputId": "37ae7010-6967-4006-d735-b131121bdaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Remission\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Response\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbCk0rqb-WG7",
        "outputId": "664a57c6-2d29-4a08-b54a-89de2d79215d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = RandomForestClassifier()\n",
        "classifier_3 = RandomForestClassifier()\n",
        "classifier_4 = RandomForestClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sml_dAOeqax",
        "outputId": "bf996148-8d80-428d-db52-eb9bf6a56247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7891156462585034\n",
            "Macro Precision: 0.4450354609929078\n",
            "Weighted Precision: 0.7572489988903363\n",
            "Micro Recall: 0.7891156462585034\n",
            "Macro Recall: 0.3088235294117647\n",
            "Weighted Recall: 0.7891156462585034\n",
            "Micro F1 Score: 0.7891156462585034\n",
            "Macro F1 Score: 0.3143616012141909\n",
            "Weighted F1 Score: 0.7219634581208693\n",
            "Average F1 Score: 0.6084802351978545\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.8843537414965986\n",
            "Macro Precision: 0.878453781512605\n",
            "Weighted Precision: 0.8918673755216372\n",
            "Micro Recall: 0.8843537414965986\n",
            "Macro Recall: 0.6616291106938226\n",
            "Weighted Recall: 0.8843537414965986\n",
            "Micro F1 Score: 0.8843537414965986\n",
            "Macro F1 Score: 0.7135133534577707\n",
            "Weighted F1 Score: 0.8781728945207329\n",
            "Average F1 Score: 0.8253466631583674\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7857142857142857\n",
            "Macro Precision: 0.7329487957903259\n",
            "Weighted Precision: 0.777299687330913\n",
            "Micro Recall: 0.7857142857142857\n",
            "Macro Recall: 0.577131498316072\n",
            "Weighted Recall: 0.7857142857142857\n",
            "Micro F1 Score: 0.7857142857142857\n",
            "Macro F1 Score: 0.5800556979921822\n",
            "Weighted F1 Score: 0.7631012834744543\n",
            "Average F1 Score: 0.709623755726974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oldJkcW4fO8n",
        "outputId": "5da71238-0dda-469b-ad34-f828338fabd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7891156462585034\n",
            "Accuracy: 0.8843537414965986\n",
            "Accuracy: 0.7857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaboost**"
      ],
      "metadata": {
        "id": "zeWrzc7Kn24e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = AdaBoostClassifier()\n",
        "classifier_3 = AdaBoostClassifier()\n",
        "classifier_4 = AdaBoostClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOsPZ_XfeqVW",
        "outputId": "7e153a94-b8d3-47e4-e293-544d7b7d7689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7776831345826235\n",
            "Macro Precision: 0.5590769853927748\n",
            "Weighted Precision: 0.7754456542988776\n",
            "Micro Recall: 0.7776831345826235\n",
            "Macro Recall: 0.5154312062317681\n",
            "Weighted Recall: 0.7776831345826235\n",
            "Micro F1 Score: 0.7776831345826235\n",
            "Macro F1 Score: 0.5295928024191979\n",
            "Weighted F1 Score: 0.7746974924903398\n",
            "Average F1 Score: 0.6939911431640539\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.7819420783645656\n",
            "Macro Precision: 0.7298966149847932\n",
            "Weighted Precision: 0.7881288609832049\n",
            "Micro Recall: 0.7819420783645656\n",
            "Macro Recall: 0.5967042638904072\n",
            "Weighted Recall: 0.7819420783645656\n",
            "Micro F1 Score: 0.7819420783645656\n",
            "Macro F1 Score: 0.6221392591766022\n",
            "Weighted F1 Score: 0.761364967896662\n",
            "Average F1 Score: 0.7218154351459433\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7904599659284497\n",
            "Macro Precision: 0.7604241570628223\n",
            "Weighted Precision: 0.7935891888354718\n",
            "Micro Recall: 0.7904599659284497\n",
            "Macro Recall: 0.7169486636256615\n",
            "Weighted Recall: 0.7904599659284497\n",
            "Micro F1 Score: 0.7904599659284497\n",
            "Macro F1 Score: 0.7337758778861535\n",
            "Weighted F1 Score: 0.7902242405130802\n",
            "Average F1 Score: 0.7714866947758944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkswMu5_-jzy",
        "outputId": "25c846fb-16fc-4bb3-a954-e498d6df49af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7776831345826235\n",
            "Accuracy: 0.7819420783645656\n",
            "Accuracy: 0.7904599659284497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = AdaBoostClassifier()\n",
        "classifier_3 = AdaBoostClassifier()\n",
        "classifier_4 = AdaBoostClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJBtZJxhe5NB",
        "outputId": "3bad1a11-225c-40d3-d7b4-c407439a84e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7346938775510204\n",
            "Macro Precision: 0.44269943019943025\n",
            "Weighted Precision: 0.7428610190514952\n",
            "Micro Recall: 0.7346938775510204\n",
            "Macro Recall: 0.4190730837789662\n",
            "Weighted Recall: 0.7346938775510204\n",
            "Micro F1 Score: 0.7346938775510203\n",
            "Macro F1 Score: 0.42575081336549236\n",
            "Weighted F1 Score: 0.7369802101125825\n",
            "Average F1 Score: 0.6324749670096984\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.7925170068027211\n",
            "Macro Precision: 0.8273638841687907\n",
            "Weighted Precision: 0.8188721927456908\n",
            "Micro Recall: 0.7925170068027211\n",
            "Macro Recall: 0.649914536087438\n",
            "Weighted Recall: 0.7925170068027211\n",
            "Micro F1 Score: 0.7925170068027211\n",
            "Macro F1 Score: 0.6930182034982971\n",
            "Weighted F1 Score: 0.78167575837843\n",
            "Average F1 Score: 0.755736989559816\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7755102040816326\n",
            "Macro Precision: 0.7070939155757193\n",
            "Weighted Precision: 0.7730591298640166\n",
            "Micro Recall: 0.7755102040816326\n",
            "Macro Recall: 0.6688258084120432\n",
            "Weighted Recall: 0.7755102040816326\n",
            "Micro F1 Score: 0.7755102040816326\n",
            "Macro F1 Score: 0.6839857197500955\n",
            "Weighted F1 Score: 0.7732405342603699\n",
            "Average F1 Score: 0.7442454860306992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvVVSLEGfRH9",
        "outputId": "7429683a-fa70-4d33-ef64-1b2dcb6cacc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7346938775510204\n",
            "Accuracy: 0.7925170068027211\n",
            "Accuracy: 0.7755102040816326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "_JSmqpSslWCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = DecisionTreeClassifier()\n",
        "classifier_3 = DecisionTreeClassifier()\n",
        "classifier_4 = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHEVC510e5HP",
        "outputId": "5f6e9621-7ce5-45c9-8e67-71b5e2ee9254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Remission\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Response\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp8T7sk4-vez",
        "outputId": "2e068acb-29fa-47e5-ecbe-515e429a7f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = DecisionTreeClassifier()\n",
        "classifier_3 = DecisionTreeClassifier()\n",
        "classifier_4 = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEoUuyuOe5Cz",
        "outputId": "8eedf01b-ce94-48b0-fff5-139644909281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.7346938775510204\n",
            "Macro Precision: 0.49855054302422724\n",
            "Weighted Precision: 0.7329706352047919\n",
            "Micro Recall: 0.7346938775510204\n",
            "Macro Recall: 0.47484402852049906\n",
            "Weighted Recall: 0.7346938775510204\n",
            "Micro F1 Score: 0.7346938775510203\n",
            "Macro F1 Score: 0.46260806345412864\n",
            "Weighted F1 Score: 0.7292950575343048\n",
            "Average F1 Score: 0.6421989995131513\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.9693877551020408\n",
            "Macro Precision: 0.9537953339230429\n",
            "Weighted Precision: 0.9703266645003866\n",
            "Micro Recall: 0.9693877551020408\n",
            "Macro Recall: 0.9513776541961577\n",
            "Weighted Recall: 0.9693877551020408\n",
            "Micro F1 Score: 0.9693877551020408\n",
            "Macro F1 Score: 0.949955022722059\n",
            "Weighted F1 Score: 0.969220689128513\n",
            "Average F1 Score: 0.9628544889842042\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.7551020408163265\n",
            "Macro Precision: 0.6598484039713889\n",
            "Weighted Precision: 0.759917033964221\n",
            "Micro Recall: 0.7551020408163265\n",
            "Macro Recall: 0.6852801904231344\n",
            "Weighted Recall: 0.7551020408163265\n",
            "Micro F1 Score: 0.7551020408163265\n",
            "Macro F1 Score: 0.6701080432172869\n",
            "Weighted F1 Score: 0.7567680133277801\n",
            "Average F1 Score: 0.7273260324537979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkRwBzFjfSNg",
        "outputId": "e099e9b1-2a84-4671-a961-3615d5278c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7346938775510204\n",
            "Accuracy: 0.9693877551020408\n",
            "Accuracy: 0.7551020408163265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**"
      ],
      "metadata": {
        "id": "rA9_twq9oEqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = GradientBoostingClassifier()\n",
        "classifier_3 = GradientBoostingClassifier()\n",
        "classifier_4 = GradientBoostingClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcneKDede4_D",
        "outputId": "bd1d84c0-bb13-4769-8a3a-965e326acbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.9931856899488927\n",
            "Macro Precision: 0.9977678571428572\n",
            "Weighted Precision: 0.9932465320029203\n",
            "Micro Recall: 0.9931856899488927\n",
            "Macro Recall: 0.9825927817500851\n",
            "Weighted Recall: 0.9931856899488927\n",
            "Micro F1 Score: 0.9931856899488927\n",
            "Macro F1 Score: 0.9900184432730978\n",
            "Weighted F1 Score: 0.9931346203909478\n",
            "Average F1 Score: 0.9921129178709794\n",
            "\n",
            " Remission\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.9846678023850085\n",
            "Macro Precision: 0.988327721661055\n",
            "Weighted Precision: 0.9847217203264903\n",
            "Micro Recall: 0.9846678023850085\n",
            "Macro Recall: 0.9746956858663095\n",
            "Weighted Recall: 0.9846678023850085\n",
            "Micro F1 Score: 0.9846678023850085\n",
            "Macro F1 Score: 0.9813178236836738\n",
            "Weighted F1 Score: 0.9846268889635303\n",
            "Average F1 Score: 0.9835375050107377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Example true labels and predicted labels\n",
        "# true_labels = np.array([0, 1, 0, 1, 0])\n",
        "# predicted_labels = np.array([0, 1, 1, 1, 0])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX35PySx5YY0",
        "outputId": "40fc1e47-859e-489b-bdd3-c3ec962f057e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9931856899488927\n",
            "Accuracy: 1.0\n",
            "Accuracy: 0.9846678023850085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = GradientBoostingClassifier()\n",
        "classifier_3 = GradientBoostingClassifier()\n",
        "classifier_4 = GradientBoostingClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84JRkHNUe48P",
        "outputId": "a2e5645c-641a-41bf-c703-5646b3214cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.8333333333333334\n",
            "Macro Precision: 0.5679862306368331\n",
            "Weighted Precision: 0.8156439167749717\n",
            "Micro Recall: 0.8333333333333334\n",
            "Macro Recall: 0.4557932263814617\n",
            "Weighted Recall: 0.8333333333333334\n",
            "Micro F1 Score: 0.8333333333333334\n",
            "Macro F1 Score: 0.4924852481777161\n",
            "Weighted F1 Score: 0.8143206605794001\n",
            "Average F1 Score: 0.7133797473634832\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.9659863945578231\n",
            "Macro Precision: 0.9189716621570511\n",
            "Weighted Precision: 0.9657473760211949\n",
            "Micro Recall: 0.9659863945578231\n",
            "Macro Recall: 0.8818870672637104\n",
            "Weighted Recall: 0.9659863945578231\n",
            "Micro F1 Score: 0.9659863945578231\n",
            "Macro F1 Score: 0.8990236632822184\n",
            "Weighted F1 Score: 0.965508776248281\n",
            "Average F1 Score: 0.9435062780294409\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.8537414965986394\n",
            "Macro Precision: 0.7748925142555246\n",
            "Weighted Precision: 0.8496490400827762\n",
            "Micro Recall: 0.8537414965986394\n",
            "Macro Recall: 0.7286588505397854\n",
            "Weighted Recall: 0.8537414965986394\n",
            "Micro F1 Score: 0.8537414965986394\n",
            "Macro F1 Score: 0.7458612176354112\n",
            "Weighted F1 Score: 0.8501268636637301\n",
            "Average F1 Score: 0.8165765259659269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lQ5l2l9fTbf",
        "outputId": "bacf9ecf-b778-4b1d-ca89-52847dfbc7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "Accuracy: 0.9659863945578231\n",
            "Accuracy: 0.8537414965986394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost**"
      ],
      "metadata": {
        "id": "ct9udR4MoWq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = XGBClassifier()\n",
        "classifier_3 = XGBClassifier()\n",
        "classifier_4 = XGBClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_train)\n",
        "y_pred_2 = classifier_2.predict(X_train)\n",
        "y_pred_3 = classifier_3.predict(X_train)\n",
        "y_pred_4 = classifier_4.predict(X_train)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_train['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_train['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_train['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZTykTgLe42T",
        "outputId": "d09ab251-2e7d-4474-e4ac-de0e1f01ecf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Remission\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n",
            "\n",
            " Response\n",
            "Micro Precision: 1.0\n",
            "Macro Precision: 1.0\n",
            "Weighted Precision: 1.0\n",
            "Micro Recall: 1.0\n",
            "Macro Recall: 1.0\n",
            "Weighted Recall: 1.0\n",
            "Micro F1 Score: 1.0\n",
            "Macro F1 Score: 1.0\n",
            "Weighted F1 Score: 1.0\n",
            "Average F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Accuracy**"
      ],
      "metadata": {
        "id": "D9h5gZD_7acs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_train['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2FzTvFU7Yju",
        "outputId": "cf37d3ce-927f-4f0f-aac2-7f7553600999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# test\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Check if the column data type is object (categorical)\n",
        "    if df[column].dtype == 'object':\n",
        "        # Use LabelEncoder to transform the categorical values into numerical labels\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Separate features and labels\n",
        "features = df.drop(['UNIQUEID',\t'THERAPY', 'DRUG', 'DOSAGE', 'LHAMD','FHAMD','DHAMD','FREQUENCY', 'FHAMD_level','Difference', 'Remission', 'Response'], axis=1)\n",
        "labels = df[['FHAMD_level', 'Difference', 'Remission','Response']]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual SVM classifiers\n",
        "# classifier_1 = SVC()\n",
        "classifier_2 = XGBClassifier()\n",
        "classifier_3 = XGBClassifier()\n",
        "classifier_4 = XGBClassifier()\n",
        "\n",
        "# Train the classifiers independently\n",
        "# classifier_1.fit(X_train, y_train['FHAMD_level'])\n",
        "classifier_2.fit(X_train, y_train['Difference'])\n",
        "classifier_3.fit(X_train, y_train['Remission'])\n",
        "classifier_4.fit(X_train, y_train['Response'])\n",
        "\n",
        "# Make predictions for each output\n",
        "# y_pred_1 = classifier_1.predict(X_test)\n",
        "y_pred_2 = classifier_2.predict(X_test)\n",
        "y_pred_3 = classifier_3.predict(X_test)\n",
        "y_pred_4 = classifier_4.predict(X_test)\n",
        "\n",
        "# Evaluate the performance for each output\n",
        "# ...\n",
        "\n",
        "# Perform binary relevance prediction on the test set\n",
        "y_pred = pd.DataFrame({\n",
        "    # 'Label_1': y_pred_1,\n",
        "    'Label_2': y_pred_2,\n",
        "    'Label_3': y_pred_3,\n",
        "    'Label_4': y_pred_4\n",
        "})\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Difference\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "precision_macro = precision_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "recall_macro = recall_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "f1_micro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='micro')\n",
        "f1_macro_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='macro')\n",
        "f1_weighted_Difference = f1_score(y_test['Difference'], y_pred['Label_2'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Difference')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Difference)\n",
        "print(\"Macro F1 Score:\", f1_macro_Difference)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Difference)\n",
        "f1_scores = [f1_micro_Difference, f1_macro_Difference, f1_weighted_Difference]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "# Remission\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "precision_macro = precision_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "recall_macro = recall_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "f1_micro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='micro')\n",
        "f1_macro_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='macro')\n",
        "f1_weighted_Remission = f1_score(y_test['Remission'], y_pred['Label_3'], average='weighted')\n",
        "# Print the results\n",
        "print('\\n Remission')\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Remission)\n",
        "print(\"Macro F1 Score:\", f1_macro_Remission)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Remission)\n",
        "f1_scores = [f1_micro_Remission, f1_macro_Remission, f1_weighted_Remission]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n",
        "\n",
        "#Response\n",
        "# Calculate precision, recall, and F1 score using different average options\n",
        "precision_micro = precision_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "precision_macro = precision_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "precision_weighted = precision_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "recall_micro = recall_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "recall_macro = recall_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "recall_weighted = recall_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "f1_micro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='micro')\n",
        "f1_macro_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='macro')\n",
        "f1_weighted_Response = f1_score(y_test['Response'], y_pred['Label_4'], average='weighted')\n",
        "# Print the results\n",
        "print(\"\\n Response\")\n",
        "print(\"Micro Precision:\", precision_micro)\n",
        "print(\"Macro Precision:\", precision_macro)\n",
        "print(\"Weighted Precision:\", precision_weighted)\n",
        "print(\"Micro Recall:\", recall_micro)\n",
        "print(\"Macro Recall:\", recall_macro)\n",
        "print(\"Weighted Recall:\", recall_weighted)\n",
        "print(\"Micro F1 Score:\", f1_micro_Response)\n",
        "print(\"Macro F1 Score:\", f1_macro_Response)\n",
        "print(\"Weighted F1 Score:\", f1_weighted_Response)\n",
        "f1_scores = [f1_micro_Response, f1_macro_Response, f1_weighted_Response]\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "print(\"Average F1 Score:\", average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PrLIa-Le7T1",
        "outputId": "41690ecb-25ea-42bb-f4b4-b3a7a6c5a669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Difference\n",
            "Micro Precision: 0.8469387755102041\n",
            "Macro Precision: 0.6446498875140607\n",
            "Weighted Precision: 0.843497719672796\n",
            "Micro Recall: 0.8469387755102041\n",
            "Macro Recall: 0.45657308377896616\n",
            "Weighted Recall: 0.8469387755102041\n",
            "Micro F1 Score: 0.8469387755102041\n",
            "Macro F1 Score: 0.5065790588514396\n",
            "Weighted F1 Score: 0.8261108384858359\n",
            "Average F1 Score: 0.7265428909491599\n",
            "\n",
            " Remission\n",
            "Micro Precision: 0.9795918367346939\n",
            "Macro Precision: 0.9780860071301247\n",
            "Weighted Precision: 0.9798510313216197\n",
            "Micro Recall: 0.9795918367346939\n",
            "Macro Recall: 0.9444129471808946\n",
            "Weighted Recall: 0.9795918367346939\n",
            "Micro F1 Score: 0.9795918367346939\n",
            "Macro F1 Score: 0.9598303237020831\n",
            "Weighted F1 Score: 0.9795091005223688\n",
            "Average F1 Score: 0.9729770869863819\n",
            "\n",
            " Response\n",
            "Micro Precision: 0.8707482993197279\n",
            "Macro Precision: 0.8192060922228218\n",
            "Weighted Precision: 0.8692338097318438\n",
            "Micro Recall: 0.8707482993197279\n",
            "Macro Recall: 0.7550133785757093\n",
            "Weighted Recall: 0.8707482993197279\n",
            "Micro F1 Score: 0.8707482993197279\n",
            "Macro F1 Score: 0.7777630350385768\n",
            "Weighted F1 Score: 0.867481629243176\n",
            "Average F1 Score: 0.8386643212004935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy**"
      ],
      "metadata": {
        "id": "qsWPj_m57rVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test['Difference'], y_pred['Label_2'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Remission'], y_pred['Label_3'])\n",
        "print(\"Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(y_test['Response'], y_pred['Label_4'])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvqHhNmI7qQW",
        "outputId": "938d94ac-b033-48f3-f5e4-72eee59f8ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8469387755102041\n",
            "Accuracy: 0.9795918367346939\n",
            "Accuracy: 0.8707482993197279\n"
          ]
        }
      ]
    }
  ]
}